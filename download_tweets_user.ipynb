{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Please refer to https://github.com/analyticsbot/user-tweet-download/blob/master/README.md for instru\n",
    "ctions to the run the code and understand the variables.\n",
    "\n",
    "Author: analyticsbot\n",
    "\n",
    "Objective: Download a user's tweet, avoiding the 3200 limit imposed by Twitter API\n",
    "\n",
    "Returns: Excel csv with user tweet with corresponding values\n",
    "\"\"\"\n",
    "\n",
    "## imports\n",
    "import tweepy\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sys import platform\n",
    "import struct\n",
    "import zipfile\n",
    "import requests\n",
    "import configparser\n",
    "from dateutil.parser import parse\n",
    "import multiprocessing\n",
    "import sys\n",
    "import pytz\n",
    "import helpers\n",
    "import random\n",
    "from importlib import reload\n",
    "reload( helpers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## determine the platform and bit of the platform, load the config file\n",
    "\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    sys_platform = 'linux'\n",
    "elif platform == \"darwin\":\n",
    "    sys_platform = 'macos'\n",
    "elif platform == \"win32\":\n",
    "    sys_platform = 'windows'\n",
    "    \n",
    "bit_system = struct.calcsize(\"P\") * 8\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parse the config variables\n",
    "\n",
    "DATE_IN_PAST = config['DEFAULT']['DATE_IN_PAST']\n",
    "DAYS_IN_PAST = config['DEFAULT'].getint('DAYS_IN_PAST')\n",
    "NUM_TWEETS_TO_DOWNLOAD = config['DEFAULT'].getint('NUM_TWEETS_TO_DOWNLOAD')\n",
    "OUTPUT_FILE_NAME_SUFFIX = config['DEFAULT']['OUTPUT_FILE_NAME_SUFFIX']\n",
    "TIME_SLEEP = config['DEFAULT'].getint('TIME_SLEEP')\n",
    "TIME_SLEEP_BROWSER_CLOSE = config['DEFAULT'].getint('TIME_SLEEP_BROWSER_CLOSE')\n",
    "\n",
    "# [TWITTER]\n",
    "TWITTER_USER_NAME = config['TWITTER']['TWITTER_USER_NAME']\n",
    "CONSUMER_KEY = config['TWITTER']['CONSUMER_KEY']\n",
    "CONSUMER_SECRET = config['TWITTER']['CONSUMER_SECRET']\n",
    "ACCESS_TOKEN = config['TWITTER']['ACCESS_TOKEN']\n",
    "ACCESS_TOKEN_SECRET = config['TWITTER']['ACCESS_TOKEN_SECRET']\n",
    "TWITTER_URL = \"https://twitter.com/search?q=(from%3A{TWITTER_USER_NAME})%20until%3A{until}%20since%3A{since}&src=typed_query&f=live\"\n",
    "TWITTER_URL = TWITTER_URL.replace('{TWITTER_USER_NAME}', TWITTER_USER_NAME)\n",
    "\n",
    "# [CHROME]\n",
    "# if None, it will download from the Internet\n",
    "CHROME_GECKODRIVER_LOCATION = config['CHROME']['CHROME_GECKODRIVER_LOCATION']\n",
    "USE_CHROME = config['CHROME'].getboolean('USE_CHROME')\n",
    "NUM_THREADS_CHROME = config['CHROME'].getint('NUM_THREADS_CHROME')\n",
    "\n",
    "# [FIREFOX]\n",
    "# if None, it will download from the Internet\n",
    "FIREFOX_GECKODRIVER_LOCATION = config['FIREFOX']['FIREFOX_GECKODRIVER_LOCATION']\n",
    "USE_FIREFOX = config['FIREFOX'].getboolean('USE_FIREFOX')\n",
    "NUM_THREADS_FIREFOX = config['FIREFOX'].getint('NUM_THREADS_FIREFOX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate to Twitter\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check authentication went okay\n",
    "\n",
    "assert api.verify_credentials()\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except Exception as e:\n",
    "    print(\"Error during authentication\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## logic to determine the until when the data is to be parsed\n",
    "\n",
    "USER_CREATED_DATE = parse(api.get_user(TWITTER_USER_NAME)._json['created_at'])\n",
    "TODAY_DATE = datetime.now(USER_CREATED_DATE.tzinfo)\n",
    "\n",
    "if not DATE_IN_PAST_PARSED and not str(DAYS_IN_PAST).isdigit():\n",
    "    print ('Either DATE_IN_PAST or DAYS_IN_PAST need to be given')\n",
    "    sys.exit(1)\n",
    "    \n",
    "try:\n",
    "    DATE_IN_PAST_PARSED = parse(DATE_IN_PAST)\n",
    "    DATE_IN_PAST_PARSED = pytz.utc.localize(DATE_IN_PAST_PARSED)\n",
    "except Exception as e:\n",
    "    print (str(e))\n",
    "    DATE_IN_PAST_PARSED = False\n",
    "if DATE_IN_PAST_PARSED:\n",
    "    if (TODAY_DATE - DATE_IN_PAST_PARSED).days > DAYS_IN_PAST:\n",
    "        NEW_DAYS_IN_PAST = (TODAY_DATE - DATE_IN_PAST_PARSED).days\n",
    "    else:\n",
    "        NEW_DAYS_IN_PAST = DAYS_IN_PAST\n",
    "    if (TODAY_DATE - USER_CREATED_DATE).days < (TODAY_DATE - DATE_IN_PAST_PARSED).days:\n",
    "        NEW_DAYS_IN_PAST = (TODAY_DATE - USER_CREATED_DATE).days\n",
    "    else:\n",
    "        NEW_DAYS_IN_PAST = (TODAY_DATE - DATE_IN_PAST_PARSED).days\n",
    "    if (TODAY_DATE - USER_CREATED_DATE).days > DAYS_IN_PAST:\n",
    "        NEW_DAYS_IN_PAST = DAYS_IN_PAST\n",
    "    else:\n",
    "        NEW_DAYS_IN_PAST = (TODAY_DATE - USER_CREATED_DATE).days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get most recent 3200 tweets via Twitter API\n",
    "\n",
    "tweet_objects = []\n",
    "\n",
    "for page in tweepy.Cursor(api.user_timeline, id=TWITTER_USER_NAME, tweet_mode='extended', \\\n",
    "                          count=NUM_TWEETS_TO_DOWNLOAD).pages():\n",
    "    tweet_objects.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert the API response to a CSV file\n",
    "\n",
    "tweets_column = ['screen_name', 'text', 'created_date', 'retweet_count', 'favorite_count', \\\n",
    "                 'replies_count', 'tweet_url', 'language', 'video_url', 'video_views']\n",
    "\n",
    "def tweet_object(tweet_objects):\n",
    "    df = pd.DataFrame(columns=tweets_column)\n",
    "    count_tweets = 0\n",
    "    break_loop = False\n",
    "\n",
    "    for tweet_object in tweet_objects:\n",
    "        if break_loop:\n",
    "            break\n",
    "        for tweet in tweet_object:\n",
    "            count_tweets +=1\n",
    "            if count_tweets > NUM_TWEETS_TO_DOWNLOAD:\n",
    "                break_loop = True\n",
    "                break\n",
    "            tweet = dict(tweet._json)\n",
    "            try:\n",
    "                screen_name = tweet['user']['screen_name']\n",
    "            except:\n",
    "                screen_name = 'NA'\n",
    "            try:\n",
    "                text = tweet['full_text']\n",
    "            except:\n",
    "                text = 'NA'\n",
    "            try:\n",
    "                created_date = tweet['created_at']\n",
    "            except:\n",
    "                created_date = 'NA'\n",
    "            try:\n",
    "                retweet_count = tweet['retweet_count']\n",
    "            except:\n",
    "                retweet_count = 'NA'\n",
    "            try:\n",
    "                favorite_count = tweet['favorite_count']\n",
    "            except:\n",
    "                favorite_count = 'NA'\n",
    "            try:\n",
    "                replies_count = tweet['created_at']\n",
    "            except:\n",
    "                replies_count = 'NA'\n",
    "            try:\n",
    "                tweet_url = 'https://twitter.com/' + screen_name + '/status/' + tweet['id_str']\n",
    "            except:\n",
    "                tweet_url = 'NA'\n",
    "            try:\n",
    "                language = tweet['lang']\n",
    "            except:\n",
    "                language = 'NA'\n",
    "            try:\n",
    "                video_url = tweet['entities']['urls'][0]['expanded_url']\n",
    "            except:\n",
    "                video_url = 'NA'\n",
    "            try:\n",
    "                video_views = 'NA'\n",
    "            except:\n",
    "                video_views = 'NA'\n",
    "\n",
    "            df.loc[df.shape[0]+1] = [screen_name, text, created_date, retweet_count, favorite_count, \\\n",
    "                     replies_count, tweet_url, language, video_url, video_views]\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api = tweet_object(tweet_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## logic to find the last tweet's date from the API response\n",
    "\n",
    "last_tweet_date = parse(df_api.loc[df_api.shape[0]]['created_date'])\n",
    "\n",
    "if OUTPUT_FILE_NAME_SUFFIX == 'None':\n",
    "    OUTPUT_FILE_NAME_SUFFIX = ''\n",
    "    \n",
    "if NUM_TWEETS_TO_DOWNLOAD < 3200:\n",
    "    df_api.to_csv(TWITTER_USER_NAME + '_' + OUTPUT_FILE_NAME_SUFFIX + '_TWEETS.csv', index=False)\n",
    "    print ('Tweets for user = ', TWITTER_USER_NAME, 'downoaded. Filename = ', TWITTER_USER_NAME + '_' + OUTPUT_FILE_NAME_SUFFIX + '_TWEETS.csv')\n",
    "    print ('Total tweets downloaded', df_api.shape[0])\n",
    "    START_DAY = 0\n",
    "    END_DAY = NEW_DAYS_IN_PAST\n",
    "    GET_REPLIES_COUNT = True\n",
    "    \n",
    "else:\n",
    "    START_DAY = 0\n",
    "    END_DAY = NEW_DAYS_IN_PAST\n",
    "    GET_REPLIES_COUNT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of the firefox, chrome threads\n",
    "NUM_THREADS_FIREFOX, NUM_THREADS_CHROME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## logic to handle number of threads depending on the config file. More in the Readme file\n",
    "## https://github.com/analyticsbot/user-tweet-download/blob/master/README.md\n",
    "\n",
    "if driver_paths['chrome']:\n",
    "    if NUM_THREADS_CHROME == 0:\n",
    "        NUM_THREADS_CHROME = 1\n",
    "else:\n",
    "    NUM_THREADS_CHROME = 0\n",
    "    \n",
    "if driver_paths['firefox']:\n",
    "    if NUM_THREADS_FIREFOX == 0:\n",
    "        NUM_THREADS_FIREFOX = 1\n",
    "else:\n",
    "    NUM_THREADS_FIREFOX = 0\n",
    "        \n",
    "def split(seq, num):\n",
    "    avg = len(seq)/ float(num)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "    while last < len(seq):\n",
    "        out.append(seq[int(last):int(last + avg)])\n",
    "        last+=avg\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## distribute the selenium work into number of threads\n",
    "\n",
    "ALL_DAYS = range(START_DAY, END_DAY)\n",
    "\n",
    "NUMBER_THREADS = NUM_THREADS_CHROME + NUM_THREADS_FIREFOX\n",
    "distributed_days = split(ALL_DAYS, NUMBER_THREADS)\n",
    "BROWSER_TYPE = ['chrome']*NUM_THREADS_CHROME + ['firefox']*NUM_THREADS_FIREFOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_DAYS, NUMBER_THREADS, distributed_days, BROWSER_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to download tweet data using selenium\n",
    "\n",
    "def get_data_twitter_selenium(DAYS_THREAD, BROWSER, driver_path, TIME_SLEEP, TIME_SLEEP_BROWSER_CLOSE, THREAD):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        DAYS_THREAD - Range of days for which this thread has to function\n",
    "        BROWSER - Browser type (chrome or firefox)\n",
    "        driver_path - path of the driver\n",
    "        TIME_SLEEP - sleep time between url loads\n",
    "        TIME_SLEEP_BROWSER_CLOSE - sleep time between browser exit and start\n",
    "        THREAD - thread number\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "        \n",
    "    Output:\n",
    "        CSV file containing data acquired by this thread\n",
    "    \"\"\"\n",
    "    if BROWSER == 'chrome':\n",
    "        browser = webdriver.Chrome(executable_path = driver_path)\n",
    "    else:\n",
    "        browser = webdriver.Firefox(executable_path = driver_path)\n",
    "        \n",
    "    df = pd.DataFrame(columns=['tweet_text_material', 'text', 'replies_count', 'retweet_count', 'favorite_count', 'tweet_url',\\\n",
    "                        'created_date', 'video_url', 'video_views'])\n",
    "    df_url = pd.DataFrame(columns=['screen_name', 'url', 'start_date', 'end_date'])\n",
    "    filename = 0\n",
    "    num_tweets = 0\n",
    "    for days_to_subtract in DAYS_THREAD:\n",
    "        until = (datetime.today() - timedelta(days=days_to_subtract)).strftime('%Y-%m-%d')\n",
    "        since = (datetime.today() - timedelta(days=days_to_subtract+1)).strftime('%Y-%m-%d')\n",
    "        NEW_TWITTER_URL = TWITTER_URL.replace('{until}', until).replace('{since}', since)\n",
    "        print (NEW_TWITTER_URL)\n",
    "        \n",
    "        if (days_to_subtract+1)%5==0:\n",
    "            browser.close()\n",
    "            time.sleep(TIME_SLEEP_BROWSER_CLOSE)\n",
    "            if BROWSER == 'chrome':\n",
    "                browser = webdriver.Chrome(executable_path = driver_path)\n",
    "            else:\n",
    "                browser = webdriver.Firefox(executable_path = driver_path)\n",
    "\n",
    "        browser.get(NEW_TWITTER_URL)\n",
    "        time.sleep(TIME_SLEEP)\n",
    "\n",
    "        last_20_tweets = ['NA']*20\n",
    "        for i in range(10):\n",
    "            tweet_div = browser.find_elements_by_css_selector('.css-1dbjc4n.r-my5ep6.r-qklmqi.r-1adg3ll')\n",
    "            tweet_div_other = browser.find_elements_by_css_selector('.css-4rbku5.css-18t94o4.css-901oao.r-1re7ezh.r-1loqt21.r-1q142lx.r-1qd0xha.r-a023e6.r-16dba41.r-ad9z0x.r-bcqeeo.r-3s2u2q.r-qvutc0')\n",
    "            length = len(tweet_div_other)\n",
    "            break_ = False\n",
    "\n",
    "            for i in range(length):\n",
    "                num_tweets+=1\n",
    "                tweet_text_material = tweet_div[i].text\n",
    "                if tweet_text_material in last_20_tweets:\n",
    "                    break_ = True\n",
    "                    break\n",
    "\n",
    "                last_20_tweets[1:] = last_20_tweets[:-1]\n",
    "                last_20_tweets[0] = tweet_text_material\n",
    "\n",
    "                tweet_text, replies, rts, favs = ' '.join(tweet_text_material.split('\\n')[4:-4]), tweet_text_material.split('\\n')[-3], tweet_text_material.split('\\n')[-2], tweet_text_material.split('\\n')[-1]\n",
    "                tweet_url = tweet_div_other[i].get_attribute('href')\n",
    "                tweet_date = tweet_div_other[i].get_attribute('title')\n",
    "                \n",
    "\n",
    "                try:\n",
    "                    video_views = tweet_div[i].find_element_by_css_selector('.css-901oao.css-16my406.r-lrvibr').text\n",
    "                except:\n",
    "                    video_views = 'None'\n",
    "                try:\n",
    "                    video_url = tweet_div[i].find_element_by_tag_name('video').get_attribute('src')\n",
    "                except:\n",
    "                    video_url = 'None'\n",
    "           \n",
    "                df.loc[df.shape[0]+1] = [tweet_text_material, tweet_text, replies, rts, favs, tweet_url, tweet_date, video_url, video_views]\n",
    "                \n",
    "                if df.shape[0]>200:\n",
    "                    df['screen_name'] = tweet_text_material.split('\\n')[1][1:]\n",
    "                    df['language'] = ''\n",
    "                    df.drop_duplicates(inplace=True)\n",
    "                    df.to_csv(TWITTER_USER_NAME + '_' + OUTPUT_FILE_NAME_SUFFIX + '_' + str(filename) +  '_TWEETS_BROWSER.csv', index=False)\n",
    "                    filename+=1\n",
    "                    df = pd.DataFrame(['tweet_text_material', 'text', 'replies_count', 'retweet_count', 'favorite_count', 'tweet_url',\\\n",
    "                        'created_date', 'video_url', 'video_views'])\n",
    "            if break_:\n",
    "                break\n",
    "\n",
    "            browser.execute_script(\"return document.body.scrollHeight\")\n",
    "            time.sleep(6)\n",
    "            df_url.loc[df_url.shape[0]+1] = [tweet_text_material.split('\\n')[1][1:], NEW_TWITTER_URL, since, until]\n",
    "        \n",
    "            if random.randint(1,100)==9:\n",
    "                print (THREAD, 'alive....')\n",
    "            \n",
    "    df['screen_name'] = tweet_text_material.split('\\n')[1][1:]\n",
    "    df['language'] = ''\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.to_csv(TWITTER_USER_NAME + '_' + OUTPUT_FILE_NAME_SUFFIX + '_' + str(filename) +  '_TWEETS_BROWSER.csv', index=False)\n",
    "    browser.close()\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "## distribute work using multiprocessing\n",
    "\n",
    "threads = []\n",
    "for i in range(NUMBER_THREADS):\n",
    "    th = multiprocessing.Process(target = get_data_twitter_selenium, kwargs = {'DAYS_THREAD': distributed_days[i],\\\n",
    "                                                                              'BROWSER': BROWSER_TYPE[i],\\\n",
    "                                                                              'driver_path': driver_paths[BROWSER_TYPE[i]],\\\n",
    "                                                                              'TIME_SLEEP': TIME_SLEEP,\\\n",
    "                                                                              'TIME_SLEEP_BROWSER_CLOSE':TIME_SLEEP_BROWSER_CLOSE,\\\n",
    "                                                                              'THREAD': i+1})\n",
    "    threads.append(th)\n",
    "    th.start()\n",
    "    \n",
    "for th in threads:\n",
    "    th.join()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge the selenium csv files into one file\n",
    "\n",
    "df_browser = pd.DataFrame(columns=['tweet_text_material', 'text', 'replies_count', 'retweet_count', 'favorite_count', 'tweet_url',\\\n",
    "                        'created_date', 'video_url', 'video_views', 'screen_name', 'language'])\n",
    "files = os.listdir('.')\n",
    "for f in files:\n",
    "    if f.startswith(TWITTER_USER_NAME) and f.endswith('BROWSER.csv') and any([i.isalpha() for i in f]):\n",
    "        temp = pd.read_csv(f)\n",
    "        try:\n",
    "            temp.drop('Unnamed: 0', inplace=True, axis=1)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            temp.drop('Unnamed: 0.1', inplace=True, axis=1)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        df_browser = pd.concat([df_browser, temp], axis=0)\n",
    "        \n",
    "df_browser.to_csv(TWITTER_USER_NAME + '_' + OUTPUT_FILE_NAME_SUFFIX +  '_TWEETS_BROWSER.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate to Twitter\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "## merge API csv file and browser file\n",
    "\n",
    "if GET_REPLIES_COUNT:\n",
    "    df_api['tweet_id'] = df_api['tweet_url'].apply(lambda x:int(x.split('/')[-1][:-1]))\n",
    "    df_browser['tweet_id'] = df_browser['tweet_url'].apply(lambda x:int(x.split('/')[-1][:-1]))\n",
    "    df_api = df_api.join(df_browser[['replies_count', 'tweet_url']], how='inner', on=['tweet_id'], lsuffix='_api', rsuffix='_browser')\n",
    "    \n",
    "    df_api.to_csv(TWITTER_USER_NAME + '_' + OUTPUT_FILE_NAME_SUFFIX +  '_TWEETS_API.csv', index=False)\n",
    "else:\n",
    "    tweet_urls = df_browser['tweet_url'].tolist()\n",
    "    tweet_urls = [t.split('/')[1] for t in tweet_urls]\n",
    "    idx=0\n",
    "    df_api_2 = pd.DataFrame(columns=tweets_column)\n",
    "    while True:\n",
    "        statuses = tweet_urls[idx*100:(idx+1)*100]\n",
    "        idx+=1\n",
    "        if len(statuses)==0:\n",
    "            break\n",
    "        tweets = api.statuses_lookup(id_=statuses, tweet_mode='extended')\n",
    "        tweets_df = tweet_object(tweet_objects)\n",
    "        df_api_2 = pd.concat([df_api_2, tweets_df], axis=0)\n",
    "\n",
    "df_api_2['tweet_id'] = df_api_2['tweet_url'].apply(lambda x:int(x.split('/')[-1][:-1]))\n",
    "df_browser['tweet_id'] = df_browser['tweet_url'].apply(lambda x:int(x.split('/')[-1][:-1]))\n",
    "df_api_2 = df_api_2.join(df_browser[['replies_count', 'tweet_id']], how='inner', on='tweet_id', lsuffix='api', rsuffix='browser_')\n",
    "df_browser.to_csv(TWITTER_USER_NAME + '_' + OUTPUT_FILE_NAME_SUFFIX +  '_TWEETS_API_BROWSER.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
