{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<module 'helpers' from '/Users/i24009/Documents/Personal/projects/twitter_project/user-tweet-download/helpers.py'>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "\"\"\"\n",
    "Please refer to https://github.com/analyticsbot/user-tweet-download/blob/master/README.md for \n",
    "instructions to the run the code and understand the variables.\n",
    "\n",
    "Author: analyticsbot (analyticsbot.xyz[at]gmail[dot]com)\n",
    "\n",
    "Objective of this program: At the moment, Twitter API has a limit of 3200 while fetching tweets for a user. This program aims at bypassing that limit using\n",
    "Selenium browser\n",
    "\n",
    "Returns: Excel csv with user tweet with corresponding values\n",
    "    Tweet text – denoted by text\n",
    "    Number of replies to the tweet – denoted by replies_count\n",
    "    Number of retweets to the tweet – denoted by retweet_count\n",
    "    Number of times this tweet has been favorited – denoted by favorite_count\n",
    "    Url of the tweet – denoted by tweet_url\n",
    "    Creation date/time of the tweet – denoted by created_date\n",
    "    If a video was attached to the tweet, what is the url – denoted by video_url\n",
    "    If a video was attached to the tweet, how many times it is viewed – denoted by video_views\n",
    "    The twitter username – denoted by screen_name\n",
    "    The language of the tweet – denoted by language\n",
    "\"\"\"\n",
    "## imports\n",
    "import tweepy\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import configparser\n",
    "from dateutil.parser import parse\n",
    "import multiprocessing\n",
    "import sys\n",
    "import pytz\n",
    "import helpers\n",
    "import random\n",
    "import os\n",
    "from importlib import reload\n",
    "reload( helpers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['config.py']"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "## load the config file\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parse the config variables\n",
    "\n",
    "DATE_IN_PAST = config['DEFAULT']['DATE_IN_PAST']\n",
    "DAYS_IN_PAST = config['DEFAULT'].getint('DAYS_IN_PAST')\n",
    "NUM_TWEETS_TO_DOWNLOAD = config['DEFAULT'].getint('NUM_TWEETS_TO_DOWNLOAD')\n",
    "OUTPUT_FILE_NAME_SUFFIX = config['DEFAULT']['OUTPUT_FILE_NAME_SUFFIX']\n",
    "TIME_SLEEP = config['DEFAULT'].getint('TIME_SLEEP')\n",
    "TIME_SLEEP_BROWSER_CLOSE = config['DEFAULT'].getint('TIME_SLEEP_BROWSER_CLOSE')\n",
    "\n",
    "# [TWITTER]\n",
    "TWITTER_USER_NAME = config['TWITTER']['TWITTER_USER_NAME']\n",
    "CONSUMER_KEY = config['TWITTER']['CONSUMER_KEY']\n",
    "CONSUMER_SECRET = config['TWITTER']['CONSUMER_SECRET']\n",
    "ACCESS_TOKEN = config['TWITTER']['ACCESS_TOKEN']\n",
    "ACCESS_TOKEN_SECRET = config['TWITTER']['ACCESS_TOKEN_SECRET']\n",
    "TWITTER_URL = config['TWITTER']['TWITTER_SEARCH_URL']\n",
    "TWITTER_URL = TWITTER_URL.replace('{TWITTER_USER_NAME}', TWITTER_USER_NAME)\n",
    "\n",
    "# [CHROME]\n",
    "# if None, it will download from the Internet\n",
    "CHROME_GECKODRIVER_LOCATION = config['CHROME']['CHROME_GECKODRIVER_LOCATION']\n",
    "USE_CHROME = config['CHROME'].getboolean('USE_CHROME')\n",
    "NUM_THREADS_CHROME = config['CHROME'].getint('NUM_THREADS_CHROME')\n",
    "\n",
    "# [FIREFOX]\n",
    "# if None, it will download from the Internet\n",
    "FIREFOX_GECKODRIVER_LOCATION = config['FIREFOX']['FIREFOX_GECKODRIVER_LOCATION']\n",
    "USE_FIREFOX = config['FIREFOX'].getboolean('USE_FIREFOX')\n",
    "NUM_THREADS_FIREFOX = config['FIREFOX'].getint('NUM_THREADS_FIREFOX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'https://twitter.com/search?q=(from%3Anarendramodi)%20until%3A{until}%20since%3A{since}&src=typed_query&f=live'"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "TWITTER_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate to Twitter\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Authentication OK\n"
    }
   ],
   "source": [
    "# Check authentication went okay\n",
    "assert api.verify_credentials()\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except Exception as e:\n",
    "    print(\"Error during authentication\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## logic to determine the until when the data is to be parsed\n",
    "USER_CREATED_DATE = parse(api.get_user(TWITTER_USER_NAME)._json['created_at'])\n",
    "TODAY_DATE = datetime.now(USER_CREATED_DATE.tzinfo)\n",
    "\n",
    "if not DATE_IN_PAST and not str(DAYS_IN_PAST).isdigit():\n",
    "    print ('Either DATE_IN_PAST or DAYS_IN_PAST need to be given')\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    DATE_IN_PAST_PARSED = parse(DATE_IN_PAST)\n",
    "    DATE_IN_PAST_PARSED = pytz.utc.localize(DATE_IN_PAST_PARSED)\n",
    "except Exception as e:\n",
    "    print (str(e))\n",
    "    DATE_IN_PAST_PARSED = False\n",
    "\n",
    "if DATE_IN_PAST_PARSED:\n",
    "    if (TODAY_DATE - DATE_IN_PAST_PARSED).days > DAYS_IN_PAST:\n",
    "        NEW_DAYS_IN_PAST = (TODAY_DATE - DATE_IN_PAST_PARSED).days\n",
    "    else:\n",
    "        NEW_DAYS_IN_PAST = DAYS_IN_PAST\n",
    "    if (TODAY_DATE - USER_CREATED_DATE).days < (TODAY_DATE - DATE_IN_PAST_PARSED).days:\n",
    "        NEW_DAYS_IN_PAST = (TODAY_DATE - USER_CREATED_DATE).days\n",
    "    else:\n",
    "        NEW_DAYS_IN_PAST = (TODAY_DATE - DATE_IN_PAST_PARSED).days\n",
    "    if (TODAY_DATE - USER_CREATED_DATE).days > DAYS_IN_PAST:\n",
    "        NEW_DAYS_IN_PAST = DAYS_IN_PAST\n",
    "    else:\n",
    "        NEW_DAYS_IN_PAST = (TODAY_DATE - USER_CREATED_DATE).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get most recent 3200 tweets via Twitter API\n",
    "tweet_objects = []\n",
    "\n",
    "for page in tweepy.Cursor(api.user_timeline, id=TWITTER_USER_NAME, tweet_mode='extended', \\\n",
    "                          count=NUM_TWEETS_TO_DOWNLOAD).pages():\n",
    "    tweet_objects.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert the API response to a CSV file\n",
    "tweets_column = ['screen_name', 'text', 'created_date', 'retweet_count', 'favorite_count', \\\n",
    "                 'replies_count', 'tweet_url', 'language', 'video_url', 'video_views']\n",
    "\n",
    "def tweet_object(tweet_objects):\n",
    "    df = pd.DataFrame(columns=tweets_column)\n",
    "    count_tweets = 0\n",
    "    break_loop = False\n",
    "\n",
    "    for tweets in tweet_objects:\n",
    "        if break_loop:\n",
    "            break\n",
    "\n",
    "        for tweet in tweets:\n",
    "            count_tweets +=1\n",
    "            if count_tweets > NUM_TWEETS_TO_DOWNLOAD:\n",
    "                break_loop = True\n",
    "                break\n",
    "            tweet = dict(tweet._json)\n",
    "            try:\n",
    "                screen_name = tweet['user']['screen_name']\n",
    "            except:\n",
    "                screen_name = 'NA'\n",
    "\n",
    "            try:\n",
    "                text = tweet['full_text']\n",
    "            except:\n",
    "                text = 'NA'\n",
    "\n",
    "            try:\n",
    "                created_date = tweet['created_at']\n",
    "            except:\n",
    "                created_date = 'NA'\n",
    "\n",
    "            try:\n",
    "                retweet_count = tweet['retweet_count']\n",
    "            except:\n",
    "                retweet_count = 'NA'\n",
    "\n",
    "            try:\n",
    "                favorite_count = tweet['favorite_count']\n",
    "            except:\n",
    "                favorite_count = 'NA'\n",
    "\n",
    "            try:\n",
    "                replies_count = tweet['created_at']\n",
    "            except:\n",
    "                replies_count = 'NA'\n",
    "\n",
    "            try:\n",
    "                tweet_url = 'https://twitter.com/' + screen_name + '/status/' + tweet['id_str']\n",
    "            except:\n",
    "                tweet_url = 'NA'\n",
    "\n",
    "            try:\n",
    "                language = tweet['lang']\n",
    "            except:\n",
    "                language = 'NA'\n",
    "\n",
    "            try:\n",
    "                video_url = tweet['entities']['urls'][0]['expanded_url']\n",
    "            except:\n",
    "                video_url = 'NA'\n",
    "\n",
    "            try:\n",
    "                video_views = 'NA'\n",
    "            except:\n",
    "                video_views = 'NA'\n",
    "\n",
    "            df.loc[df.shape[0]+1] = [screen_name, text, created_date, retweet_count, favorite_count, \\\n",
    "                     replies_count, tweet_url, language, video_url, video_views]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api = tweet_object(tweet_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Tweets for user =  narendramodi downoaded. Filename =  narendramodi__TWEETS.csv\nTotal tweets downloaded 100\n"
    }
   ],
   "source": [
    "if OUTPUT_FILE_NAME_SUFFIX == 'None':\n",
    "    OUTPUT_FILE_NAME_SUFFIX = ''\n",
    "\n",
    "if NUM_TWEETS_TO_DOWNLOAD < 3200:\n",
    "    df_api.to_csv(TWITTER_USER_NAME + '_' + OUTPUT_FILE_NAME_SUFFIX + '_TWEETS.csv', index=False)\n",
    "    print ('Tweets for user = ', TWITTER_USER_NAME, 'downoaded. Filename = ', TWITTER_USER_NAME + '_' + OUTPUT_FILE_NAME_SUFFIX + '_TWEETS.csv')\n",
    "    print ('Total tweets downloaded', df_api.shape[0])\n",
    "    START_DAY = 0\n",
    "    END_DAY = NEW_DAYS_IN_PAST\n",
    "    GET_REPLIES_COUNT = True\n",
    "\n",
    "else:\n",
    "    START_DAY = 0\n",
    "    END_DAY = NEW_DAYS_IN_PAST\n",
    "    GET_REPLIES_COUNT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## logic to handle number of threads depending on the config file. More in the Readme file\n",
    "## https://github.com/analyticsbot/user-tweet-download/blob/master/README.md\n",
    "\n",
    "driver_paths = helpers.getPathDriver(config)\n",
    "\n",
    "if driver_paths['chrome']:\n",
    "    if NUM_THREADS_CHROME == 0:\n",
    "        NUM_THREADS_CHROME = 1\n",
    "else:\n",
    "    NUM_THREADS_CHROME = 0\n",
    "\n",
    "if driver_paths['firefox']:\n",
    "    if NUM_THREADS_FIREFOX == 0:\n",
    "        NUM_THREADS_FIREFOX = 1\n",
    "else:\n",
    "    NUM_THREADS_FIREFOX = 0\n",
    "\n",
    "def split(seq, num):\n",
    "    avg = len(seq)/ float(num)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "    while last < len(seq):\n",
    "        out.append(seq[int(last):int(last + avg)])\n",
    "        last+=avg\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(range(0, 5), 1, [range(0, 5)], ['firefox'])"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "## distribute the selenium work into number of threads\n",
    "ALL_DAYS = range(START_DAY, END_DAY)\n",
    "\n",
    "NUMBER_THREADS = NUM_THREADS_CHROME + NUM_THREADS_FIREFOX\n",
    "distributed_days = split(ALL_DAYS, NUMBER_THREADS)\n",
    "BROWSER_TYPE = ['chrome']*NUM_THREADS_CHROME + ['firefox']*NUM_THREADS_FIREFOX\n",
    "\n",
    "\n",
    "ALL_DAYS, NUMBER_THREADS, distributed_days, BROWSER_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to download tweet data using selenium\n",
    "def get_data_twitter_selenium(DAYS_THREAD, BROWSER, driver_path, TIME_SLEEP, TIME_SLEEP_BROWSER_CLOSE, THREAD):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        DAYS_THREAD - Range of days for which this thread has to function\n",
    "        BROWSER - Browser type (chrome or firefox)\n",
    "        driver_path - path of the driver\n",
    "        TIME_SLEEP - sleep time between url loads\n",
    "        TIME_SLEEP_BROWSER_CLOSE - sleep time between browser exit and start\n",
    "        THREAD - thread number\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Output:\n",
    "        CSV file containing data acquired by this thread\n",
    "    \"\"\"\n",
    "    if BROWSER == 'chrome':\n",
    "        browser = webdriver.Chrome(executable_path = driver_path)\n",
    "    else:\n",
    "        browser = webdriver.Firefox(executable_path = driver_path)\n",
    "\n",
    "    df = pd.DataFrame(columns=['tweet_text_material', 'text', 'replies_count', 'retweet_count', 'favorite_count', 'tweet_url',\\\n",
    "                        'created_date', 'video_url', 'video_views'])\n",
    "    df_url = pd.DataFrame(columns=['screen_name', 'url', 'start_date', 'end_date'])\n",
    "\n",
    "    filename = 0\n",
    "    num_tweets = 0\n",
    "\n",
    "    for days_to_subtract in DAYS_THREAD:\n",
    "        until = (datetime.today() - timedelta(days=days_to_subtract)).strftime('%Y-%m-%d')\n",
    "        since = (datetime.today() - timedelta(days=days_to_subtract+1)).strftime('%Y-%m-%d')\n",
    "\n",
    "        NEW_TWITTER_URL = TWITTER_URL.replace('{until}', until).replace('{since}', since)\n",
    "        print (NEW_TWITTER_URL)\n",
    "\n",
    "        if (days_to_subtract+1)%5==0:\n",
    "            browser.close()\n",
    "            time.sleep(TIME_SLEEP_BROWSER_CLOSE)\n",
    "            if BROWSER == 'chrome':\n",
    "                browser = webdriver.Chrome(executable_path = driver_path)\n",
    "            else:\n",
    "                browser = webdriver.Firefox(executable_path = driver_path)\n",
    "\n",
    "        browser.get(NEW_TWITTER_URL)\n",
    "        time.sleep(TIME_SLEEP)\n",
    "\n",
    "        last_20_tweets = ['NA']*20\n",
    "        for i in range(10):\n",
    "            the_tweet = browser.find_elements_by_tag_name('article')\n",
    "            the_tweet_meta = browser.find_elements_by_xpath(\"//article//time/parent::a\")\n",
    "            break_ = False\n",
    "\n",
    "            for i in range(len(the_tweet)):\n",
    "                num_tweets+=1\n",
    "                tweet_text_material = the_tweet[i].text\n",
    "                if tweet_text_material in last_20_tweets:\n",
    "                    break_ = True\n",
    "                    break\n",
    "\n",
    "                last_20_tweets[1:] = last_20_tweets[:-1]\n",
    "                last_20_tweets[0] = tweet_text_material\n",
    "\n",
    "                tweet_text, replies, rts, favs = ' '.join(tweet_text_material.split('\\n')[4:-4]), tweet_text_material.split('\\n')[-3], tweet_text_material.split('\\n')[-2], tweet_text_material.split('\\n')[-1]\n",
    "                tweet_url = the_tweet_meta[i].get_attribute('href')\n",
    "                tweet_date = the_tweet_meta[i].get_attribute('title')\n",
    "\n",
    "\n",
    "                try:\n",
    "                    video_views = the_tweet[i].find_element_by_css_selector('.css-901oao.css-16my406.r-lrvibr').text\n",
    "                except:\n",
    "                    video_views = 'None'\n",
    "\n",
    "                try:\n",
    "                    video_url = the_tweet[i].find_element_by_tag_name('video').get_attribute('src')\n",
    "                except:\n",
    "                    video_url = 'None'\n",
    "\n",
    "                df.loc[df.shape[0]+1] = [tweet_text_material, tweet_text, replies, rts, favs, tweet_url, tweet_date, video_url, video_views]\n",
    "\n",
    "                if df.shape[0]>200:\n",
    "                    df['screen_name'] = tweet_text_material.split('\\n')[1][1:]\n",
    "                    df['language'] = ''\n",
    "                    df.drop_duplicates(inplace=True)\n",
    "                    df.to_csv(TWITTER_USER_NAME + '_' + OUTPUT_FILE_NAME_SUFFIX + '_' + str(filename) +  '_TWEETS_BROWSER.csv', index=False)\n",
    "                    filename+=1\n",
    "                    df = pd.DataFrame(['tweet_text_material', 'text', 'replies_count', 'retweet_count', 'favorite_count', 'tweet_url',\\\n",
    "                        'created_date', 'video_url', 'video_views'])\n",
    "            if break_:\n",
    "                break\n",
    "\n",
    "        df_url.loc[df_url.shape[0]+1] = [tweet_text_material.split('\\n')[1][1:], NEW_TWITTER_URL, since, until]\n",
    "    df['screen_name'] = tweet_text_material.split('\\n')[1][1:]\n",
    "    df['language'] = ''\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.to_csv(TWITTER_USER_NAME + '_' + OUTPUT_FILE_NAME_SUFFIX + '_' + str(filename) +  '_TWEETS_BROWSER.csv', index=False)\n",
    "    df_url.to_csv(TWITTER_USER_NAME + '_' + OUTPUT_FILE_NAME_SUFFIX + '_TWEETS_BROWSER_URLS.csv', index=False)\n",
    "    browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## distribute work using multiprocessing\n",
    "threads = []\n",
    "for i in range(NUMBER_THREADS):\n",
    "    th = multiprocessing.Process(target = get_data_twitter_selenium, kwargs = {'DAYS_THREAD': distributed_days[i],\\\n",
    "                                                                              'BROWSER': BROWSER_TYPE[i],\\\n",
    "                                                                              'driver_path': driver_paths[BROWSER_TYPE[i]],\\\n",
    "                                                                              'TIME_SLEEP': TIME_SLEEP,\\\n",
    "                                                                              'TIME_SLEEP_BROWSER_CLOSE':TIME_SLEEP_BROWSER_CLOSE,\\\n",
    "                                                                              'THREAD': i+1})\n",
    "    threads.append(th)\n",
    "    th.start()\n",
    "\n",
    "for th in threads:\n",
    "    th.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate to Twitter\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge API csv file and browser file\n",
    "if GET_REPLIES_COUNT:\n",
    "    df_api['tweet_id'] = df_api['tweet_url'].apply(lambda x:int(x.split('/')[-1][:-1]))\n",
    "    df_browser['tweet_id'] = df_browser['tweet_url'].apply(lambda x:int(x.split('/')[-1][:-1]))\n",
    "    df_api = df_api.join(df_browser[['replies_count', 'tweet_url']], how='inner', on=['tweet_id'], lsuffix='_api', rsuffix='_browser')\n",
    "\n",
    "    df_api.to_csv(TWITTER_USER_NAME + '_' + OUTPUT_FILE_NAME_SUFFIX +  '_TWEETS_API.csv', index=False)\n",
    "else:\n",
    "    tweet_urls = df_browser['tweet_url'].tolist()\n",
    "    tweet_urls = [t.split('/')[1] for t in tweet_urls]\n",
    "    idx=0\n",
    "    df_api_2 = pd.DataFrame(columns=tweets_column)\n",
    "    while True:\n",
    "        statuses = tweet_urls[idx*100:(idx+1)*100]\n",
    "        idx+=1\n",
    "        if len(statuses)==0:\n",
    "            break\n",
    "        tweets = api.statuses_lookup(id_=statuses, tweet_mode='extended')\n",
    "        tweets_df = tweet_object(tweet_objects)\n",
    "        df_api_2 = pd.concat([df_api_2, tweets_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api_2['tweet_id'] = df_api_2['tweet_url'].apply(lambda x:int(x.split('/')[-1][:-1]))\n",
    "df_browser['tweet_id'] = df_browser['tweet_url'].apply(lambda x:int(x.split('/')[-1][:-1]))\n",
    "df_api_2 = df_api_2.join(df_browser[['replies_count', 'tweet_id']], how='inner', on='tweet_id', lsuffix='api', rsuffix='browser_')\n",
    "df_browser.to_csv(TWITTER_USER_NAME + '_' + OUTPUT_FILE_NAME_SUFFIX +  '_TWEETS_API_BROWSER.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37264bit22c554dde7334c1a9c735d66aa86284c",
   "display_name": "Python 3.7.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}